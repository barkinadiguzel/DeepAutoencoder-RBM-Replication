# ğŸ§® Deep Autoencoder RBM Replication

This repository contains a **minimal and faithful replication** of the **Deep Autoencoder with RBM pretraining** in PyTorch.  
The goal is to reproduce the core ideas of **layer-wise pretraining**, **deep encoding-decoding**, and **dimensionality reduction** in a clean, modular, and easy-to-extend form.  

- Deep autoencoder with **RBM pretraining** ğŸŒ€  
- Layer-wise encoding-decoding architecture ğŸŒ¿  
- Captures hierarchical features from raw inputs ğŸ§   
- Suitable for dimensionality reduction, feature extraction, and unsupervised representation learning âœ¨
-  
**Paper reference:** [Hinton & Salakhutdinov, 2006 â€“ Reducing the Dimensionality of Data with Neural Networks](https://www.cs.toronto.edu/~hinton/absps/science.pdf) ğŸ§©

---

## ğŸŒ„ Overview â€“ Deep Autoencoder Architecture

![Figure Mix](images/figmix.jpg)

This overview highlights the structure:

- Input data (e.g., 784-dimensional vectors for MNIST) is **compressed through stacked RBM layers** 
- **Encoding Network** reduces dimensionality to the code layer  
- **Code Layer** represents the most compact feature representation 
- **Decoding Network** reconstructs the input from the code, aiming to minimize reconstruction error 
- Layer-wise pretraining with RBMs helps the network **initialize weights efficiently** before fine-tuning.  

---

## ğŸ“ Key Mathematical Idea

The Deep Autoencoder is trained to **minimize reconstruction error**:

For input $$x$$, encoding function $$f$$, decoding function $$g$$:

$$
h = f(x) = \sigma(W_{enc} x + b_{enc})
$$

$$
\hat{x} = g(h) = \sigma(W_{dec} h + b_{dec})
$$

$$
\mathcal{L}_{recon} = \frac{1}{N} \sum_{i=1}^{N} ||x_i - \hat{x}_i||^2
$$

RBM pretraining uses **contrastive divergence**:

$$
\Delta W = \eta (\langle v h \rangle_{data} - \langle v h \rangle_{recon})
$$

$$
\Delta b_v = \eta \langle v - v_{recon} \rangle, \quad
\Delta b_h = \eta \langle h - h_{recon} \rangle
$$

where:

- $$v$$: visible units, $$h$$: hidden units  
- $$\langle \cdot \rangle$$ denotes expectation over data or model  
- $$\eta$$ is the learning rate  

Pretraining initializes the weights $$W$$ before fine-tuning the full autoencoder with **backpropagation**.  

---

## ğŸ“¦ File Structure

```bash
DeepAutoencoder-RBM-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ rbm_layer.py
â”‚   â”‚   â”œâ”€â”€ encoder_layer.py
â”‚   â”‚   â”œâ”€â”€ decoder_layer.py
â”‚   â”‚   â”œâ”€â”€ code_layer.py
â”‚   â”‚   â””â”€â”€ utils_layers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ rbm_stack.py
â”‚   â”‚   â”œâ”€â”€ encoder_network.py
â”‚   â”‚   â””â”€â”€ decoder_network.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ deep_autoencoder.py
â”‚   â”‚
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
